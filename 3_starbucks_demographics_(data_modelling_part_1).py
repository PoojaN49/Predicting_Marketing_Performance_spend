# -*- coding: utf-8 -*-
"""3. Starbucks Demographics (Data Modelling - Part 1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bmre169kOeOvrxM2NsREncVOtzpJgWRX

# Determining Starbucks Demographics

In this notebook I will use the data cleaning performed in the processing notebook to attempt to determine what the core demographics of Starbucks membership are in a effort to predict spend and ultimately improve the targeting of Starbucks offers;

This process will involve running an unsupervised learning method to cluster the data into groups based on information provided in their profiles including age, gender, income and length of membership. It will also take into account how much users normally spend at Starbucks on a per transaction basis.

First, I will perform some further processing of the data to prepare it for the model (standard scalars etc.). Then I will run a K-means ml model to cluster the data and determine what the optimum number of clusters describes the demographics appropriately. After this I will describe some interesting findings from the results to gain data understanding before the final step of this project in the second half of the modeling to predict user spend.

### Imports
"""

import sbpkg

# import general functions
import pandas as pd
import numpy as np
import json

# import functions for modelling
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# plotting functions
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import date
import matplotlib.style as style
import matplotlib.gridspec as gridspec
from pandas.plotting import register_matplotlib_converters
from matplotlib import ticker

# Saving the ML model
import joblib

# import the cleaning package
import sbpkg as sb

"""### Functions"""



"""### Global Variables"""

# read in the different datasources
portfolio_df = pd.read_json('/content/portfolio.json', lines=True)
profile_df = pd.read_json('/content/profile.json', lines=True)
transcript_df = pd.read_json('/content/transcript.json', lines=True)

clean_trans_df

"""### Run Cleaning Functions"""

import clbg as cl

# run the initial cleaning on each dataset
clean_prof_df = cl.clean_profile_data(profile_df)

clean_port_df = sb.clean_portfolio_data(portfolio_df)
clean_trans_df = sb.clean_transcript_data(transcript_df)

# calculates the uninfluenced transactions for the modeling
uninflunced_trans = sb.norm_transactions(clean_trans_df, clean_port_df)

# process the user data to create the modeling input
user_data = sb.user_transactions(clean_prof_df, uninflunced_trans)
user_data.head()

transcript_df

"""### Demographics Modelling

Now that I have the demographics input I can apply some final processing prior to modelling.

Here I will apply feature scaling to the input dataset. This is mainly to adjust the income column as it has a very large range of 0-120,000 which if not scaled will impact the behaviour of clustering. This is because the most amount of reduction in distance to cluster center will come from just grouping users based on income.
"""

# remove unwanted columns
demographics_input = user_data.drop(columns=['member joined','person', 
                                             'total transactions', 'total spend'])

# Apply feature scaling to the demographics data.
scaler = StandardScaler()
scaler.fit(demographics_input)
input_demo_data = scaler.transform(demographics_input)

# run k-means over different cluster counts to see the best number of groups to split our demographic data into
# creates plot
fig, ax = plt.subplots(figsize=(18,9))

# loop through all of the different cluster counts
ad = {}
cluster_counts = [2,3,4,5,6,7,8,9,10,12,14,16,18,20,22]
for x in cluster_counts:
    kmeans = KMeans(n_clusters=x, max_iter=1000).fit(input_demo_data)
    ad[x] = kmeans.inertia_
    
# plot the average distance to the center of the clusters 
ax.plot(list(ad.keys()), list(ad.values()), linewidth=3, alpha=0.6)

# label axis
ax.set_xlabel("Number of cluster", fontsize=18)
ax.set_ylabel("Average Distance", fontsize=18)

# Removes spines and changes layout to tight
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# show plot
plt.show()

"""Plotting the average distance to centroid  by the number of clusters is the best way to determine what the optimum number of cluster should be. This optimum point is called the 'knee' of the graph and shows where the most amount of information gain for the least amount of clusters are. On the graph above we can see a relatively clear knee at around 6 clusters and another small one at 8 where there are changes in information. I've decided to use 8 clusters as the biggest deciding factor on splitting clusters was found be 'Gender' probably as it's a categorical variable. Having a larger number of clusters means I get some splits on other features such as 'Income' and 'age' also."""

# create and run the model
final_kmeans = KMeans(n_clusters=8, max_iter=1000).fit(input_demo_data)
predict = final_kmeans.predict(input_demo_data)

# add the pedictions for the clustering onto the original dataset
demographics_output = demographics_input
demographics_output['cluster'] = predict

"""We can now use the model to predict which clusters users will be grouped into as shown above. I have performed an in depth analysis below into how each of the clusters has been split out.

### Demographics Analysis
"""

demographics_output.groupby('cluster').mean()

demographics_output.groupby('cluster')['income'].describe()

# creates plot
fig, ax = plt.subplots(figsize=(18,9))

for demographic in [0,1,2,3,4,5,6,7]:
    # input data 
    df = demographics_output[demographics_output['cluster'] == demographic]

    # creates a list of colors
    colors = ['blue','red','cyan','green','violet','pink','orange','purple']

    # plot selected demographics
    ax.scatter(df['spend per day'], df['income'], 
               color=colors[demographic], label=f'Demographic {demographic}',
              alpha=0.6)
    
# show the plot legend
ax.legend(loc=4, frameon=False)

# set x label
ax.set_xlabel('Spend Per Day {$}', fontsize=18)

# set y label
ax.set_ylabel('Income ($)', fontsize=18)

# set size of the axis params
ax.tick_params(axis="x", labelsize=12)
ax.tick_params(axis="y", labelsize=12)

# Adds the comma into y labels
ax.get_yaxis().set_major_formatter(
ticker.FuncFormatter(lambda x, p: format(int(x), ',')))

# Removes spines and changes layout to tight
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()

# show/close
plt.show()

demographics_output.groupby('cluster')['age'].describe()

# creates plot
fig, ax = plt.subplots(figsize=(18,9))

for demographic in [0,1,2,3,4,5,6,7]:
    # input data 
    df = demographics_output[demographics_output['cluster'] == demographic]

    # creates a list of colors
    colors = ['blue','red','cyan','green','violet','pink','orange','purple']

    # plot selected demographics
    ax.scatter(df['age'], df['income'], 
               color=colors[demographic], label=f'Demographic {demographic}',
              alpha=0.6)
    
# show the plot legend
ax.legend(loc=4, frameon=False)

# set x label
ax.set_xlabel('Age', fontsize=18)

# set y label
ax.set_ylabel('Income ($)', fontsize=18)

# set size of the axis params
ax.tick_params(axis="x", labelsize=12)
ax.tick_params(axis="y", labelsize=12)

# Adds the comma into y labels
ax.get_yaxis().set_major_formatter(
ticker.FuncFormatter(lambda x, p: format(int(x), ',')))

# Removes spines and changes layout to tight
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
plt.tight_layout()

# show/close
plt.show()

"""Above shows that we have 8 clusters with the majority being divided primarily on Gender as only two of the clusters contain users with mixed genders. Other key dividing features in the data appear to be spend, age and income.

There are two clusters with males only 1 & 7 respectively. They appear to have been split on all of the three factors above of spend, age and income. This makes sense as younger people tend to have a lower income and therefore will probably spend less on Starbucks coffee. Similarly there are two clusters for women that are split along similar lines (spend, age and income) these are clusters 0 & 5.

Users who put 'other' or have an unknown gender have been put into separate clusters of their own 4 and 2 respectively. It's worth noting that users who didn't specify their gender had very little spend at all this could be because they aren't as engaged users who haven't filled in their personal information.

Finally there are two mixed clusters with both men & women in number 3 & 6. Cluster number 3 appears to show very well off but high spending users. Whereas cluster 6 appears to show young, lower earning users who have had membership for a long time. This could indicate that they are regular customers who spend an average amount each week.

### Output the demographics model

Now that we have our demographic model we can save it and create a function to preform the clustering on any new dataset provided but also to use in future notebooks.
"""

joblib.dump(final_kmeans, 'kmeans_demographic_model.pkl')

# Create a function below to read in the model and predict the demographic for other datasets
def predict_demographic(profile_data, demographic_model='kmeans_demographic_model.pkl'):
    """
    this can be used to predict the demographics of group of consumers
    """
    # Reads the volume model 
    final_kmeans = joblib.load(demographic_model)
    
    # remove unwanted columns
    profile_data_input = profile_data.drop(columns=['member joined',
                                              'person', 
                                              'total transactions', 
                                              'total spend'])
    
    # process the profile data
    scaler = StandardScaler()
    scaler.fit(profile_data_input)
    input_demo_data = scaler.transform(profile_data_input)
    
    # predict the demographics   
    predictions = final_kmeans.predict(input_demo_data)
    
    # add the pedictions for the clustering onto the original dataset
    updated_dataframe = profile_data
    updated_dataframe['demographic'] = predictions
    
    return updated_dataframe

uninflunced_trans = sb.norm_transactions(clean_trans_df, clean_port_df)

# process the user data to create the modeling input
user_data = sb.user_transactions(clean_prof_df, uninflunced_trans)

# load in the user spend by day
spd = sb.spend_per_day(clean_trans_df, clean_port_df)
spd.reset_index(inplace=True)

# predict the demographic of all the users in the data
predictions = sb.predict_demographic(user_data)
predictions.head()

input_data

demographics = predictions[['person','demographic']]

# merge the two datasets so that we have the demographic data for each person
input_data = spd.merge(demographics, on=['person'])
input_data.head()

# only keep the first 23 days so that the last 7 can be used for modeling
input_data = input_data[input_data['transaction_time'] < 24]

# sum the spend & number of offers that 
input_data = input_data.groupby(['transaction_time','person']).sum()
input_data.tail()

def create_dummy_days(data):
    """
    this module creates dummies depending on the day of the week
    (this is useful as users behaviour will be different on a weekday vs weekend)
    """
    # add dummies for the days of the week 
    day_of_week = pd.DataFrame(list(data['transaction_time']))
    for n in [1,2,3,4,5,6,7]:
        day_of_week = day_of_week.replace((n+7), n).replace((n+7*2), n).replace((n+7*3), n).replace((n+7*4), n)   
    day_of_week = pd.DataFrame([str(x) for x in day_of_week.iloc[:,0]])
    input_data_test = pd.concat([data, pd.get_dummies(day_of_week)], axis=1, join='inner')
    
    return input_data_test

# To perform testing on the data I will only keep one demographic for now
data = input_data[input_data['demographic'] == 0]

# reset the index
data.reset_index(inplace=True)

# add dummies for the days of the week 
input_data_test = create_dummy_days(data)

# keep only the columns needed
input_data_test = input_data_test.drop(columns=['transaction_time','person','demographic'])
input_data_test.head()

# import functions for modelling
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

# split the data again into testing and training data
X_train, X_test, y_train, y_test = train_test_split(input_data_test.iloc[:,1:],
                                                   input_data_test.iloc[:,0],
                                                   test_size=.1, 
                                                   random_state=42)

# change the input to a dmatrix to improve optimsation speed for testing
# Note DMatrix was choosen to replace the standard xgboost to reduce the training times
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# calculate the baseline RMSE error, this will be used for evaluation of the model
baseline_pred = np.ones(y_test.shape) * np.mean(y_train)
rmse_baseline = np.sqrt(mean_squared_error(y_test, baseline_pred))

print(f"Baseline RMSE is {rmse_baseline}")

# set up the intial parameters for the xgboosting parameterisation
params = {
    'max_depth':5,
    'min_child_weight': 1,
    'eta':.1,
    'subsample': 1,
    'colsample_bytree': 1,
    'gamma':0.1,
    'objective':'reg:squarederror',
}

# set the rmse value as a very high value
rmse = 1000

# set_ best parameters as nothing
best_parms = None

# sets values for max_depth & min_child_weight
testing_prams = [
    (max_depth, min_child_weight)
    for max_depth in range(7,12)
    for min_child_weight in range(3,8)
]

# loop through both of the variablles
for max_depth, min_child_weight in testing_prams:
    # update the parameters
    params['max_depth'] = max_depth
    params['min_child_weight'] = min_child_weight
    
    # Run CV
    results = xgb.cv(
        params,
        dtrain,
        num_boost_round=999,
        seed=14,
        nfold=5,
        metrics={'rmse'},
        early_stopping_rounds=10
    )
    
    # save the best rmse score
    best_rmse = results['test-rmse-mean'].min()
    
    # print out the results
    print(f"max_depth = {max_depth} | "
          f"min_child_weight = {min_child_weight} | "
          f"rmse = {best_rmse}")
    
    # set the best parameters if they improve on the current rmse
    if best_rmse < rmse:
        rmse = best_rmse
        best_params = (max_depth, min_child_weight)
        
print(f"Best params: "
      f"max_depth = {best_params[0]} | "
      f"min_child_weight = {best_params[1]} | "
      f"rmse = {rmse}")

# set the new best parameters based on the testing
params['max_depth'] = 7
params['min_child_weight'] = 6

# set the rmse value as a very high value
rmse = 1000

# set_ best parameters as nothing
best_parms = None

# sets values for max_depth & min_child_weight
testing_prams = [
    (subsample, colsample_bytree)
    for subsample in [i/10. for i in range(5,11)]
    for colsample_bytree in [i/10. for i in range(5,11)]
]

# loop through both of the variablles
for subsample, colsample_bytree in testing_prams:
    # update the parameters
    params['subsample'] = subsample
    params['colsample_bytree'] = colsample_bytree
    
    # Run CV
    results = xgb.cv(
        params,
        dtrain,
        num_boost_round=999,
        seed=14,
        nfold=5,
        metrics={'rmse'},
        early_stopping_rounds=10
    )
    
    # save the best rmse score
    best_rmse = results['test-rmse-mean'].min()
    
    # print out the results
    print(f"subsample = {subsample} | "
          f"colsample_bytree = {colsample_bytree} | "
          f"rmse = {best_rmse}")
    
    # set the best parameters if they improve on the current rmse
    if best_rmse < rmse:
        rmse = best_rmse
        best_params = (subsample, colsample_bytree)
        
print(f"Best params: "
      f"subsample = {best_params[0]} | "
      f"colsample_bytree = {best_params[1]} | "
      f"rmse = {rmse}")

params['subsample'] = 0.5
params['colsample_bytree'] = 0.5

# set the rmse value as a very high value
rmse = 1000

# set_ best parameters as nothing
best_parms = None

# loop through both of the variablles
for eta in [.3, .2, .1, .05, .01, .005]:
    # update the parameters
    params['eta'] = eta
    
    # Run CV
    results = xgb.cv(
        params,
        dtrain,
        num_boost_round=999,
        seed=14,
        nfold=5,
        metrics={'rmse'},
        early_stopping_rounds=10
    )
    
    # save the best rmse score
    best_rmse = results['test-rmse-mean'].min()
    
    # print out the results
    print(f"eta = {eta} | "
          f"rmse = {best_rmse}")
    
    # set the best parameters if they improve on the current rmse
    if best_rmse < rmse:
        rmse = best_rmse
        best_params = (eta)
        
print(f"Best params: "
      f"eta = {best_params} | "
      f"rmse = {rmse}")

params['eta'] = 0.05

# test the number of boosted rounds needed to get the best model
best_model = xgb.train(
    params,
    dtrain,
    num_boost_round=999,
    evals=[(dtest, "Test")],
    early_stopping_rounds=10
)

# test the predictions created above
pred = best_model.predict(dtest)
print(np.sqrt(mean_squared_error(pred, y_test)))

params

def create_spend_model(spend_data, demographics_data, model_demographic):
    """
    this function is used create a model that can predict the price for a given demographic
    """
    # drop everything except the demographic from the prediction data
    demographics = predictions[['person','demographic']]

    # merge the two datasets so that we have the demographic data for each person
    input_data = spd.merge(demographics, on=['person'])
    input_data.head()

    # only keep the first 23 days so that the last 7 can be used for modeling
    input_data = input_data[input_data['transaction_time'] < 24]

    # sum the spend & number of offers that are influenced 
    input_data = input_data.groupby(['transaction_time','person']).sum()
    
    # To perform testing on the data I will only keep one demographic for now
    data = input_data[input_data['demographic'] == model_demographic]

    # reset the index
    data.reset_index(inplace=True)
    
    # add dummies for the days of the week 
    input_data_test = create_dummy_days(data)

    # keep only the columns needed for modeling
    input_data_test = input_data_test.drop(columns=['transaction_time','person','demographic'])
    
    # split the data again into testing and training data
    X_train, X_test, y_train, y_test = train_test_split(input_data_test.iloc[:,1:],
                                                        input_data_test.iloc[:,0],
                                                        test_size=.1, 
                                                        random_state=14)
    
    
    # creates model using the XGB classifier    
    model = xgb.XGBRegressor(max_depth=7,
                min_child_weight=5,
                subsample=1.0,
                colsample_bytree=0.5,
                objective='reg:squarederror',
                n_estimators=13,
                learning_rate=0.2)
    
    # fit the model to the training data    
    model.fit(X_train, y_train)
    
    # calculate the rmse error
    test_pred = model.predict(X_test)
    print(test_pred)
    mod_rmse = np.sqrt(mean_squared_error(test_pred, y_test))
    
    # calculate baseline rmse
    mean_train = np.mean(y_train)
    baseline_pred = np.ones(y_test.shape) * mean_train
    base_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))
    
    # save the model ready for the predictions
    joblib.dump(model, f"xgboost_price_model_{model_demographic}.pkl")
    print(f"xgboost_price_model_{model_demographic}.pkl")
    
    # print rmse compaired to the base level
    print(f"Baseline RMSE is {str(base_rmse)}")
    print(f"Model RMSE: {str(mod_rmse)}")

for demographic in [0,1,2,3,4,5,6,7]:
    create_spend_model(spd, predictions, demographic)

# create the model for each of the different demographics:
for demographic in [0,1,2,3,4,5,6,7]:
    create_spend_model(spd, predictions, demographic)

def predict_spend(input_data, model_demographic):
    """
    this function predicts the spend of users based on the model made for there demographic
    """    
    # load in the model needed to predict spend on the analysis
    demo_model = joblib.load(f"xgboost_price_model_{model_demographic}.pkl")
    
    # keep only the demographic data related to the model to be used in this section
    data = input_data[input_data['demographic'] == model_demographic]
    
    # reset the index
    data.reset_index(inplace=True)
    
    # add dummies for the days of the week 
    input_data_test = create_dummy_days(data)

    # keep only the columns needed
    input_data_test = input_data_test.drop(columns=['transaction_time','person','spend','demographic'])
    
    # calculate the prediction based on the input date
    prediction = demo_model.predict(input_data_test)
    
    # attach the prediction to the original filtered df
    input_data_test['prediction'] = prediction
    
    return input_data_test

input_data_test

user_data.shape

predictions_one = predict_spend(input_data, 1)
predictions_one.shape

predictions_one.iloc[:,-1].max()

input_data2=(0,0,0,0,0,0,0,0,0,0,0,0,6.0)

uninflunced_trans = sb.norm_transactions(clean_trans_df, clean_port_df)

# process the user data to create the modeling input
user_data = sb.user_transactions(clean_prof_df, uninflunced_trans)

# load in the user spend by day
spd = sb.spend_per_day(clean_trans_df, clean_port_df)
spd.reset_index(inplace=True)

# predict the demographic of all the users in the data
predictions = sb.predict_demographic(user_data)

# drop everything except the demographic from the prediction data
demographics = predictions[['person','demographic']]

# merge the two datasets so that we have the demographic data for each person
input_data = spd.merge(demographics, on=['person'])

# sum the spend & number of offers that 
input_data = input_data.groupby(['transaction_time','person']).sum()
input_data.head()

def plot_spend(input_data, model_demographic, color, fill=False):
    """
    sets up the data for plotting
    """
    # keep only the data for selected demographic 
    ip_df = input_data[input_data['demographic'] == model_demographic]
    
    # reset index    
    ip_df.reset_index(inplace=True)
        
    # add dummies for the days of the week 
    df = sb.create_dummy_days(ip_df)

    # create one dataset with the first 23 days (the training data)
    df_23 = df[df['transaction_time'] < 24]
    
    # add the data together across the demographic for total spend    
    spend_23d = df_23.groupby(['transaction_time']).sum()
    
    # plots the 23 days of training data used for modeling   
    ax.plot(spend_23d.iloc[:,0], color=color, alpha=0.6, 
            ls='-', linewidth=3, label=f'Training Data for demographic {model_demographic}')

    # create a second dataset with the last 7 days (validation dataset)
    df_7 = df[df['transaction_time'] > 22]
    
    # add the data together across the demographic for total spend        
    spend_7d = df_7.groupby(['transaction_time']).sum()    

    # plot the spend over time
    ax.plot(spend_7d.iloc[:,0], color=color, alpha=0.6, 
            ls=':', linewidth=3, label=f'Validation Data for demographic {model_demographic}')    

    # create prediction dataset
    predictions = df
    pred_data = sb.predict_spend(df, model_demographic)
    predictions['prediction'] = list(pred_data.iloc[:,-1])

    # group all of the data for the first 23 days into one value for the demographic
    pred_data = predictions.groupby(['transaction_time']).sum()
    
    # plot the spend over time
    ax.plot(pred_data.iloc[:,-1], color=color, alpha=0.4, 
            ls='--', linewidth=2, label='Predictions')
    
    # create a basline as if no offers were ran
    baseline_df = df.drop(columns='prediction')
    for col in baseline_df.columns[:-8]:
        baseline_df[col].values[:] = 0
    baseline_data = sb.predict_spend(baseline_df, model_demographic)
    predictions['prediction_bl'] = list(baseline_data.iloc[:,-1])

    # group all of the data for the first 23 days into one value for the demographic
    base_data = predictions.groupby(['transaction_time']).sum()
    
    # plot the spend over time
    ax.plot(base_data.iloc[:,-1], color='black', alpha=0.4, 
            ls='--', linewidth=2, label='Baseline Spend')
    
    if fill:
        # get all the orignal data        
        orig = df.groupby(['transaction_time']).sum()
        
        # plot the area filled in representing the uplift       
        ax.fill_between(x=orig.index, 
                        y1=base_data.iloc[:,-1], y2=orig.iloc[:,0], alpha=0.05, color=color)
        
        print(orig.iloc[:,0].sum() - base_data.iloc[:,-1].sum())

for demographic in [0,1,2,3,4,5,6,7]:
    # creates plot
    fig, ax = plt.subplots(figsize=(18,9))

    # creates a list of colors
    colors = ['blue','red','cyan','green','violet','pink','orange','purple']

    # plot selected demographics
    plot_spend(input_data, demographic, colors[demographic], fill=True)

    # show the plot legend
    ax.legend(loc=2, frameon=False)

    # set the x tick limits
    ax.set_xlim(0, 30)

    # set the x ticks
    ax.set_xticks(list(range(1,31)))

    # set x label
    ax.set_xlabel('Offer Period (Days)', fontsize=18)

    # set y label
    ax.set_ylabel('Consumer Spend ($)', fontsize=18)

    # set size of the axis params
    ax.tick_params(axis="x", labelsize=12)
    ax.tick_params(axis="y", labelsize=12)

    # Adds the comma into y labels
    ax.get_yaxis().set_major_formatter(
    ticker.FuncFormatter(lambda x, p: format(int(x), ',')))

    # Removes spines and changes layout to tight
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    plt.tight_layout()

    # show/close
    plt.show()

predictions # predicting demographic

predictions_one # predicting spend

